{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When is a sequence model needed?\n",
    "\n",
    "France won the 2018 world cup, The French president was present and he congratulated --------- national team for their win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Sequence Models are ones where we want the state to be memorized and passed on to the next step**\n",
    "\n",
    "Said differently, the order and the sequence of the data matter and should be remembered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\"> Examples of Sequence Problems</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![]( images/sequence depiction.PNG)\n",
    "<caption><center> <u>Jason Brownlee</u></center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/Convolution into LSTM for images.PNG\" style=\"width: 800px;\"/>\n",
    "<caption><center> <u><font size=1>Show and Tell: A Neural Image Caption Generator, 2014</font></u></center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/Translation using LSTM.PNG\"style=\"width: 800px;\"/>\n",
    "<caption><center> <u><font size=1>Sequence to Sequence Learning with Neural Networks, 2014</font></u></center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![]( images/Time series plot.PNG)\n",
    "<caption><center> <u>Weather autocorelation plot</u></center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <b>Conceptual view of RNN</b></center>\n",
    "\n",
    "![]( images/simplernn.PNG)\n",
    "\n",
    "<caption><center> <u>Jason Brownlee</u></center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <b>Unrolling an RNN over time</b></center>\n",
    "\n",
    "![]( images/unrolledrnn.PNG)\n",
    "\n",
    "<caption><center> <u>Jason Brownlee</u></center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** But RNN have a major downside: they seem to \"forget\" this internal state when the steps/sequence becomes long**\n",
    "\n",
    "This is because of something called vanishing gradients, which is a common problem for vanilla deep convulutions\n",
    "\n",
    "We will peel this off in the next few slides`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** So what is this thing I am calling vanishing gradient and why is it a problem**\n",
    "\n",
    "- Machine Learning at it's core learns through an objective seeking method\n",
    "- The objective most of the times is to minimize error between a target and the prediction from the algorithm\n",
    "- This means finding the minimum of the error function (cost function), where total error is minimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Here's what an RNN really looks like **\n",
    "![]( images/rnn_step_forward.png)\n",
    "<caption><center> <u>Andrew Ng </u></center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <b>Typical Cost Function</b></center>\n",
    "\n",
    "<caption><center> <u>Andrew Ng </u></center></caption>\n",
    "<img src=\"images/optimsurface.png\"style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "The goal is to learn the coeficients $W$ and $b$ by minimizing the cost function. \n",
    "\n",
    "We update the coeficient $W$ by applying the following formula  $ W = W - \\alpha \\text{ } dW$.  \n",
    "\n",
    "$\\alpha$ here is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "as part of the gradient descent method, partial derivatives otherwise known as gradients have to be computed for every pass of the data ( this is what is referred to as Back Propagation)\n",
    "\n",
    "![]( images/rnn_cell_backprop.png)\n",
    "\n",
    "\n",
    "<caption><center> <u>Andrew Ng </u></center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<!--NAVIGATION-->\n",
    "##### <[Contents](Index.ipynb) | [Collecting the Data](_02_Collecting_Data.ipynb) >"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
